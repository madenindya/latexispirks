%!TEX root = skripsi.tex
%-----------------------------------------------------------------------------%
\chapter{\babDua}
%-----------------------------------------------------------------------------%
Pada bab ini, dijelaskan mengenai studi literatur yang dilakukan. Studi literatur yang dilakukan digunakan sebagai dasar konsep dan teknik penelitian. Dipaparkan pula berbagai istilah dan metode yang digunakan dalam penelitian.

%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{WordNet}
%-----------------------------------------------------------------------------%
WordNet adalah kamus leksikal yang tersimpan secara digital dan digunakan untuk berbagai keperluan komputasi \citep{paper.miller}. Pembuatan WordNet dilatarbelakangi keperluan mendapatkan \textit{sense} atau arti semantik suatu kata. Informasi tersebut perlu disimpan dan dapat dibaca oleh mesin. WordNet pertama dibuat oleh \cite{paper.miller} berbasis Bahasa Inggris dan sekarang dikenal dengan nama Princeton WordNet (PWN). WordNet menyimpan informasi dalam bentuk database dimana setiap entry-nya adalah pasangan \textit{synset} dan arti semantiknya (\textit{sense}). Set sinonim (\textit{synset}) adalah himpunan kata yang memiliki arti yang sama atau saling berelasi \textit{synonym}. 

WordNet mengandung beberapa kelas kata seperti kata benda (\textit{noun}), kata kerja (\textit{verb}), kata sifat (\textit{adjective}), dan kata keterangan (\textit{adverb}). WordNet juga menyimpan informasi mengenai relasi semantik antar \textit{synset}. Relasi yang disimpan adalah \textit{synonymy}, \textit{antonymy}, \textit{hyponymy}, \textit{hypernym}, \textit{meronymy}, \textit{holonymy}, \textit{troponymy}, dan \textit{entailment}.

%-----------------------------------------------------------------------------%
\subsection{Indonesian WordNet}
%-----------------------------------------------------------------------------%
Penelitian mengenai WordNet Bahasa Indonesia pernah dilakukan sebelumnya oleh \cite{paper.putraarfanmanurung} dan \cite{paper.margarethamanurung}. Indonesian WordNet (IWN) dibangun menggunakan metode mapping antara WordNet yang sudah ada ke dalam Bahasa Indonesia \citep{paper.putraarfanmanurung}. WordNet yang digunakan sebagai dasarnya adalah Princeton WordNet. Synset dalam PWN akan dipetakan ke dalam entry Kamus Besar Bahasa Indonesia (KBBI), sehingga menghasilkan hasil yang berkualitas baik secara cepat dan mudah. Penelitian tersebut menghasilkan 1441 synset dan 3074 sense. Relasi semantik antar synset diperoleh dengan memetakan IWN synset dengan PWN synset, sehingga relasi yang dimiliki dalam PWN dapat diturunkan.

%-----------------------------------------------------------------------------%
\subsection{WordNet Bahasa}
%-----------------------------------------------------------------------------%
Pengembangan WordNet untuk Bahasa Indonesia juga dilakukan oleh Nanyang Technology University (NTU) sejak tahun 2011 dan diberi nama WordNet Bahasa \citep{paper.bond}. WordNet ini telah diintegrasi dengan salah satu \textit{tools} NLP berbasis Python yaitu nltk sehingga dapat dengan mudah digunakan dalam komputasi. WordNet Bahasa juga memanfaatkan PWN untuk mendapatkan relasi semantik antar \textit{synset}. Pada penelitian ini, dimanfaatkan \textit{tools} tersebut untuk mendapatkan \textit{seed} relasi semantik antar kata dalam Bahasa Indonesia.

%-----------------------------------------------------------------------------%
\subsection{Kelemahan WordNet Bahasa Indonesia}
%-----------------------------------------------------------------------------%
Hingga saat ini, relasi semantik antar kata yang dimiliki oleh WordNet Bahasa Indonesia merupakan hasil turunan dari relasi semantik WordNet Princeton. Sayangnya, pemanfaatan PWN untuk mendapatkan relasi semantik kata Bahasa Indonesia menemui beberapa hambatan. WordNet Bahasa Indonesia menjadi sangat tergantung dengan struktur PWN untuk mendapatkan relas semantik suatu \textit{synset}. Selain itu, beberapa \textit{synset} Bahasa Indonesia juga tidak dapat dipetakan secara tepat ke \textit{synset} PWN, beberapa kata kehilangan arti atau mendapat arti yang kurang tepat. Hal ini menyebabkan pasangan kata relasi semantik Bahasa Indonesia yang dihasilkan terlihat kurang baik. Untuk itu, dicetuskanlah penelitian untuk mengekstrak relasi semantik dalam Bahasa Indonesia secara mandiri. 

%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Relasi}
%-----------------------------------------------------------------------------%
Relasi menggambarkan hubungan atau koneksi yang dimiliki oleh suatu hal dengan yang lain (KBBI). Dalam bidang matematika, relasi memetakan suatu anggota dari himpunan satu ke himpunan lain sesuai dengan hubungan yang didefinisikan. Dalam penelitian ini, relasi yang diperhatikan adalah relasi semantik antar kata. Domainnya adalah kata-kata yang tergabung dalam kelas kata benda (\textit{noun}).

Satu relasi dapat terdiri dari beberapa entitas dan dituliskan dalam bentuk tuple $t = (e_1, e_2, ..., e_n)$ dimana $e_i$ adalah suatu entitas yang memiliki relasi $r$ dalam dokumen $D$ \citep{article.backbadaskar}. Relasi sinonim dapat ditulis dalam notasi tersebut. Banyak pula relasi yang hanya menghubungkan antar dua entitas (relasi biner), seperti \textit{terletak-di(Universitas Indonesia, Depok)} atau \textit{ditulis-oleh(Habis Gelap Terbitlah Terang, RA Kartini)}.

%-----------------------------------------------------------------------------%
\subsection{Relasi Semantik}
%-----------------------------------------------------------------------------%
Semantik adalah arti (\textit{sense}) dari suatu kata. Umumnya informasi tersebut disimpan dalam kamus bahasa. Relasi semantik adalah hubungan yang dimiliki antar kata berdasarkan arti atau makna dari kata tersebut. Beberapa relasi semantik adalah sebagai berikut \citep{paper.miller}.
\begin{itemize}
  \item \textit{Synonymy} adalah relasi antar kata dimana dua kata yang berbeda memiliki arti yang sama. Semua kelas kata dapat memiliki relasi \textit{synonymy}. Dalam WordNet, relasi ini direpresentasikan dalam bentuk \textit{synset} dan bersifat simetris. Sebagai contoh 'makan', 'melahap', dan 'menyantap' memiliki makna yang sama.
  \item \textit{Antonymy} adalah yang menggambarkan arti yang saling berkebalikan antar kata. Umumnya relasi ini digunakan pada kelas kata sifat (\textit{adverb}) dan kata keterangan (\textit{adjective}). Sama seperti synonymy, relasi ini memiliki sifat simetris. Sebagai contoh kata 'tinggi' memiliki makna yang berkebalikan dengan kata 'pendek'.
  \item \textit{Hyponymy} adalah relasi yang menyatakan hubungan kata yang lebih khusus. Sementara untuk kata yang lebih umum dikenal dengan relasi \textit{hypernym}. Kedua relasi ini diperuntukan kelas kata benda (\textit{noun}) dan umumnya satu kata memiliki hanya satu \textit{hypernym}. Kedua relasi ini bersifat transitif, sehingga dapat digambarkan dalam bentuk hirarki. Sebagai contoh kucing, ikan, kelinci (\textit{hyponymy}) adalah binatang (\textit{hypernym}). Binatang adalah \textit{hyponym} dari makhluk hidup. Sehingga dapat dikatakan pula bahwa kucing, ikan, kelinci (\textit{hyponym}) adalah makhluk hidup (\textit{hypernym}).
  \item \textit{Meronymy} dan \textit{holonym} adalah relasi yang menyatakan hubungan bagian satu dengan yang lain, dimana \textit{meronym} menyatakan sub-bagian dan \textit{holonym} menyatakan bagian yang lebih besar. Seperti relasi hyponym-hypernym, relasi \textit{meronym-holonym} bersifat transitif dan dapat digambarkan dalam bentuk hirarki. Dalam WordNet, relasi ini dibagi ke dalam tiga bagian yaitu \textit{part-meronym}, \textit{member-meronym}, dan \textit{substance-meronym}. Sebagai contoh sebuah sel (\textit{holonym}) memiliki nukleus, ribosom, mitokondria (\textit{meronym}).
  \item \textit{Troponymy} adalah relasi seperti \textit{hyponymy-hypernymy} yang khusus untuk kelas kata kerja (\textit{verb}). Dalam Bahasa Inggris, contoh kata yang memiliki relasi ini adalah 'stroll' dan 'walk'.
\end{itemize}

%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Relation Extraction}
%-----------------------------------------------------------------------------%
\textit{Relation extraction} adalah cabang dari \textit{Information Extraction} (IE) yang berfokus pada proses ekstraksi relasi antar kata. Proses ini berusaha mengekstrak informasi terstruktur dengan definisi yang diinginkan dari teks dokumen atau resource yang tidak terstruktur. Beberapa contoh penelitian ini seperti \textit{named entity recognition} (NER) yang mengetahui apakah suatu entitas adalah orang, organisasi, atau lokasi \citep{paper.bikel}. 

%-----------------------------------------------------------------------------%
\subsection{Semantic Relation Extraction}
%-----------------------------------------------------------------------------%
\textit{Semantic relation extraction} mengkhususkan pada proses ekstraksi relasi semantik antar kata. Penelitian dalam bidang ini sudah banyak dilakukan dengan berbagai metode. Salah satu metode populer untuk mendapatkan relasi semantik satu domain bahasa adalah menggunakan \textit{pattern extraction} dan \textit{pattern matching} seperti yang telah dilakukan pada penelitian (Hearst, 1992), (Ruiz-Casado dkk, 2005), dan (Arnold dan Rahm, 2014). Penelitian lain memanfaatkan distribusi kata untuk memperoleh \textit{semantic distance} antar kata. \textit{Semantic distance} adalah nilai yang merepresentasikan kedekatan antar kata berdasarkan semantiknya.

Penelitian yang dilakukan oleh Hearst merupakan salah satu penelitian awal ekstraksi relasi semantik yang menggunakan metode \textit{pattern extraction} dan \textit{matching}. Hearst menggunakan \textit{lexico-syntactic pattern} untuk mendapatkan pasangan kata relasi tanpa membutuhkan pengetahuan sebelumnya dan dapat diaplikasikan dalam teks apapun. Pada awal, Hearst mendifiniskan dua \textit{pattern} berdasarkan hasil observasi dari teks. Selanjutnya, \textit{pattern} baru didapat menggunakan langkah berikut. 
\begin{enumerate}
  \item Tentukan relasi yang akan diamati dan kumpulkan entitas yang menggambarkan relasi tersebut.
  \item Dalam teks dokumen, cari lokasi dimana entitas-entitas tersebut berada dan simpan kata-kata diantaranya (lingkungan).
  \item Cari kesamaan dari teks yang terekstraksi dan bentuk suatu \textit{pattern} baru.
  \item Jika \textit{pattern} baru telah terbukti benar, gunakan \textit{pattern} tersebut untuk mendapatkan entitas baru.
\end{enumerate}
Proses evaluasi dilakukan dengan membandingkan entitas yang dihasilkan dengan synset dalam WordNet. 

Penelitian yang dilakukan oleh Ruiz-Casado dkk, memanfaatkan WordNet dan Wikipedia sebagai korpus untuk mendapatkan pasangan kata relasi. Pertama dilakukan \textit{entry sense disambiguation} yang merupakan tahap \textit{pre-processing} untuk memetakan setiap entri dalam Wikipedia dengan \textit{synset} dalam WordNet. Tahap berikutnya adalah ekstraksi \textit{pattern} antara dua konsep. Jika terdapat dua konsep yang saling berhubungan dan memiliki suatu relasi semantik dalam WordNet maka kalimat yang mengandung dua konsep tersebut akan disimpan. Proses tersebut menghasilkan daftar relasi semantik dengan masing-masing memiliki \textit{pattern} di dalamnya. Dari banyak \textit{pattern} yang dihasilkan, proses selanjutnya adalah \textit{pattern generalisation} yang bertujuan membuat pattern yang lebih umum. Tahap ini memanfaatkan algoritma \textit{edit-distance} dengan modifikasi. Setelah mendapatkan \textit{pattern}, tahapan terakhir adalah menggunakannya ke dalam korpus untuk mendapatkan entitas baru.

Penelitian yang dilakukan Arnold dan Rahm juga memanfaatkan korpus Wikipedia dan menggunakan \textit{pattern}. Selain \textit{hypernym-hyponymy} dan \textit{holonym-meronymy}, penelitian ini juga mengidentifikasi relasi \textit{synonymy}. Kalimat definisi pada Wikipedia di-\textit{parse} menggunakan \textit{Finite State Machine} (FSM) dan konsep-konsep baru diekstrak menggunakan \textit{pattern} yang telah didefinisikan. Penelitian lain yang dilakukan oleh Sumida dan Torisawa [9] memanfaatkan struktur internal Wikipedia dalam mengekstraksi \textit{pattern} relasi untuk Bahasa Jepang. 

%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Korpus}
%-----------------------------------------------------------------------------%
Korpus adalah data yang dengan format yang dapat dibaca oleh mesin dan memiliki fungsi spesifik (Atkins et al., 1991).

%-----------------------------------------------------------------------------%
\subsection{Semantic Relation Extraction}
%-----------------------------------------------------------------------------%
Saat ini, belum ada korpus independen yang menggambarkan relasi semantik antar kata dalam Bahasa Indonesia. Relasi semantik Bahasa Indonesia yang ada saat ini didapatkan dari hasil relasi turunan pemetaan \textit{synset} Bahasa Indonesia dengan PWN. 

Korpus relasi semantik yang ingin dibuat menyimpan informasi dalam bentuk pasangan kata yang berelasi serta relasi yang menghubungkannya. Selanjutnya, pasangan kata berelasi akan disebut sebuah \textit{pair}. Untuk relasi yang bersifat transitif seperti \textit{hypernym-hyponym}, ada identifikasi kata mana yang merupakan \textit{hypernym} dan mana yang merupakan \textit{hyponym}. Relasi \textit{hypernym} dan \textit{hyponym} adalah relasi bersifat transitif yang dapat direpresentasikan dalam bentuk hirarki. Pada penelitian ini, korpus \textit{pair} yang diperoleh merepresentasikan hubungan yang berjarak tepat satu dari yang lain. 

%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Wikipedia}
%-----------------------------------------------------------------------------%
Wikipedia adalah ensiklopedia terbuka yang memuat berbagai bahasa dan merupakan hasil kolaborasi berbagai penulis (Doneyer dan Gallinari, 2006). Wikipedia adalah salah satu korpus teks dokumen terbesar yang disimpan secara \textit{online} yang dapat diakses dan diunduh secara bebas. Wikipedia dikelola oleh organisasi nonprofit bernama Wikimedia Foundation. Pada tahun 2009, jumlah kontributor Wikipedia Bahasa Indonesia telah mencapai 2.502 pengguna aktif. Walau ditulis oleh berbagai narasumber, informasi yang dimuat dalam Wikipedia dibuat secara terstruktur dengan bahasa yang formal. Wikipedia juga memuat informasi umum terbaru (Arnold dan Rahm, 2014).

%-----------------------------------------------------------------------------%
\subsection{Korpus Wikipedia}
%-----------------------------------------------------------------------------%
Wikipedia Bahasa Indonesia saat ini telah memuat lebih dari 400.000 artikel dari berbagai domain. Artikel-artikel yang disimpan dalam Wikipedia dapat diunduh secara gratis dalam bentuk \textit{dumps} dengan format XML. Beberapa tipe \textit{dump} dapat dipilih, diantarnya adalah halaman seluruh artikel, halaman artikel beserta \textit{revision history}, daftar judul artikel, dan lainnya. Secara berkala, Wikimedia membuat \textit{dump} terhadap seluruh artikel terakhir yang disimpan untuk setiap bahasa. Pada situs yang menyediakan pengunduhan data Wikipedia, terdapat tanggal unik yang menyatakan tanggal terakhir data tersebut di-\textit{update}.

%-----------------------------------------------------------------------------%
\subsection{Wikipedia sebagai Lexical Semantic Resource}
%-----------------------------------------------------------------------------%
Artikel dalam Wikipedia terdiri dari berbagai domain kategori dan memuat berbagai \textit{noun} umum. Walau ditulis secara kolaboratif, Wikipedia memuat artikel secara terstruktur dimana pada paragraf pertama umumnya berisi kalimat-kalimat definisi menganai topik yang sedang dibahas. Bahasa yang ditulis juga cukup formal dan terstruktur. Selain itu, sudah banyak penelitian ekstraksi relasi semantik dengan metode pattern yang menggunakan artikel Wikipedia seperti [8], [7]. 


%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Semi Supervised}
%-----------------------------------------------------------------------------%
Dalam machine learning terdapat dua tipe pendekatan yang umum digunakan yaitu supervised dan unsupervised learning. Supervised menggunakan data berlabel sebagai data training maupun testing. Dari kedua data tersebut, dibentuk suatu classifier yang dapat memenuhi segala kasus yang mungkin terjadi. Data testing digunakan untuk menguji kebaikan classifier yang terbentuk. Unsupervised menggunakan data yang tidak diberi label sama sekali dan berusaha untuk menemukan pola yang sama untuk suatu kumpulan data tertentu (10). Pendekatan lain yang merupakan kombinasi antara supervised dan unsupervised learning adalah semi supervised learning. 
Semi supervised adalah pendekatan machine learning dimana informasi supervised data diberikan tidak untuk seluruh data. Sebagian data merupakan data berlabel sementara sebagian lainnya belum memiliki label. Beberapa metode penerapan semi supervised adalah bootstrapping (self training), mixture models, graph based methods, co-training, dan multiview learning.
(10) A Survey on Semi-Supervised Learning Techniques. V. Jothi Prakash, Dr. L.M. Nithya. International Journal of Computer Trends and Technology. 2014 (http://www.ijcttjournal.org/Volume8/
number-1/IJCTT-V8P105.pdf)
(12) MITPress - Semi Supervised Learning

%-----------------------------------------------------------------------------%
\subsection{Bootstrapping}
%-----------------------------------------------------------------------------% 
Model bootstrapping merupakan salah satu model semi supervised learning yang paling umum digunakan. Bootstrapping menggunakan data berlabel berukuran kecil dan data tidak berlabel berukuran jauh lebih besar. Proses anotasi data tidak berlabel dilakukan secara bertahap melalui sejumlah iterasi. Dari data training berlabel, dibentuk suatu classifier yang kemudian digunakan untuk menganotasi data tidak berlabel. Sejumlah k data baru yang merupakan hasil pelabelan, dimasukkan ke dalam kelompok data berlabel. Proses tersebut dilakukan secara berulang, sehingga semakin lama iterasi jumlah data berlabel akan bertambah. 
Terdapat dua algoritma bootstrapping yang pernah digunakan untuk proses pattern extraction dan matching yaitu Meta-Bootstrapping dan Basilisk (Rillof dkk, 2003). Keduanya digunakan untuk mengelompokan kata ke dalam suatu kategori semantik jika diberikan korpus teks yang belum dianotasi dan suatu seed. Seed didefinisikan sebagai korpus kata yang sudah diketahui kategori semantiknya. Secara umum, proses ini akan mencari pattern berdasarkan seed yang diberikan. Dari pattern yang dihasilkan dan teks yang belum dianotasi, diekstrak entitas baru dan dikelompokan berdasarkan kategori semantiknya. Kata-kata tersebut akan digabungkan ke dalam korpus kata berelas.

%-----------------------------------------------------------------------------%
\subsection{Meta Bootstrapping}
%-----------------------------------------------------------------------------% 
Berikut adalah beberapa proses (Riloff dan Jones, 1999) yang dijalankan algoritma meta bootstrapping jika diberikan seed berukuran kecil yang berasal dari suatu kategori semantik dan korpus yang belum dianotasi.
Mengekstraksi pattern secara otomatis dengan menerapkan syntactic template.
Untuk setiap pattern akan diberi bobot berdasarkan jumlah seed yang menghasilkan pattern.
Diambil pattern terbaik dan seluruh seed lama yang merepresentasikan pattern maupun seed baru yang berhasil diekstrak disimpan.
Dilakukan pembobotan ulang untuk setiap pattern menggunakan seed lama dan baru.
Proses diatas dinamakan mutual bootstrapping dan setelah proses tersebut selesai, semua entitas baru hasil ekstraksi dievaluasi. Pembobotan entitas baru diberikan berdasarkan jumlah pattern yang mengekstrak kata tersebut. Lima kata terbaik diterima dan dimasukkan ke kamus (korpus) kata berelasi untuk selanjutnya diproses ulang.

%-----------------------------------------------------------------------------%
\subsection{Basilisk}
%-----------------------------------------------------------------------------% 
Algoritma Basilisk (Thelen dan Riloff, 2002) juga memanfaatkan pattern dan seed dalam membangun korpus untuk suatu kategori semantik tertentu. Beberapa tahapan yang dijalankan adalah:
Secara otomatis membentuk pattern dan memberi bobot berdasarkan jumlah seed yang menghasilkan pattern. Pattern terbaik dimasukan ke dalam Pattern Pool.
Untuk setiap entitas baru yang terekstraksi dari pattern, dimasukan ke dalam Candidate Word Pool. Pemberian bobot dilakukan berdasarkan jumlah pattern yang mengekstraksi dan asosiasi kumulatif kata dengan seed.
Sepuluh kata terbaik diambil dan dimasukan ke dalam kamus (korpus) yang kemudian digunakan untuk iterasi selanjutnya. 
Kateogori semantik untuk proses ini bisa lebih dari satu. Basilisk memberi bobot berdasarkan informasi kolektif dari kumpulan pattern yang mengekstrak kata tersebut. Sementara Meta-Bootstrapping hanya mengambil satu pattern terbaik dan mengelompokkan seluruh kata yang terekstrak dari pattern ke dalam kategori semantik yang sama. Dari hasil penelitian komparatif yang pernah dilakukan (Rillof dkk, 2003), didapatkan Basilisk mengungguli performa Meta-Bootstrapping. 
(11) Learning Subjective Noun using Extraction Pattern Bootstrapping. Rillof, Ellen, etc. Seventh Conference on Natural Language Learing.  (https://www.cs.utah.edu/~riloff/pdfs/conll03.pdf)
[19] A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Context (https://www.cs.utah.edu/~riloff/pdfs/emnlp02-thelen.pdf)


%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Pattern}
%-----------------------------------------------------------------------------%
Pattern adalah suatu bentuk yang dapat merepresentasikan kumpulan data berukuran besar. Pada text dokumen, pattern dapat terbentuk secara eksplisit ataupun implisit. Secara eksplisit seperti lexico-syntactic pattern yang terbentuk dari kata-kata di dalam dokumen tersebut. Sementara pattern terbentuk secara implisit jika dilhat dari kemiripan vektor yang merepresentasika dokumen atau kata-kata di dalam dokumen tersebut.

%-----------------------------------------------------------------------------%
\subsection{Lexico-syntactic Pattern}
%-----------------------------------------------------------------------------% 
Lexico-syntactic pattern adalah pattern yang hanya memanfatkan kata-kata dalam korpus dokumen dan dimanfaatkan untuk proses string matching. Salah satu pattern leksikal yang terkenal adalah Hearst Pattern yang merepresentasikan pola-pola untuk mendapatkan relasi hyponymy dari dokumen (Hearst, 1992). Menurut Hearst, beberapa syarat yang harus dipenuhi suatu lexico-syntactic pattern yang baik adalah sebagai berikut.
Kemunculannya sering pada teks dalam berbagai domain sehingga dapat mengekstrak banyak entitas.
Merepresentasikan relasi yang diinginkan sehingga hasil ekstraksi juga benar.
Dapat dikenali tanpa membutuhkan pengetahuan sebelumnya sehingga dapat dibentuk dalam situasi apapun.


%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
\section{Pattern Extraction dan Matching}
%-----------------------------------------------------------------------------%
Pattern extraction atau pattern recognition adalah salah satu cabang dalam machine learning yang berusaha mencari pola kemiripan tertentu dari kumpulan data yang diberikan. Pattern matching adalah proses untuk mencocokkan suatu pattern dengan kumpulan data yang belum dianotasi. Penelitian ekstraksi relsasi semantik menggunakan metode pattern matching telah dilakukan oleh (Hearst, 1992), (Ruiz-Casado dkk, 2005), (Arnold dan Rahm, 2014), dan (Sumida dan Torisawa, [9]).
Banyaknya penelitian menggunakan metode pattern matching dan extraction menjadi salah satu alasan penggunaan metode tersebut untuk mengekstrak relasi kata dalam Bahasa Indonesia. Pada penelitian ini, relasi semantik yang diekstrak dibatasi hanya untuk relasi hypernym-hyponym. 


2.9 Sequence Analysis
Dalam bidang bioinformatik, suquence analysis digunakan untuk mengetahui apakah suatu DNA atau RNA mengandung sequnce tertentu. Pada penelitian ini, pattern yang dihasilkan adalah pattern leksikal yang merupakan deretan kata-kata. Algoritma seperti standard trie dan suffix tree dimanfaatkan secara berurut untuk proses pattern extraction dan matching.

2.9.1 Standard Trie
Standard Trie untuk suatu himpunan string S adalah ordered tree dengan ketentuan berikut.
Setiap node, selain root, diberi label sebuah character
Children dari sebuah node terurut sesuai alphabet
Path dari eksternal node hingga root membentuk suatu string dalam S.
Standard Trie membutuhkan memori sebesar O(n) dimana n adalah total ukuran string dalam S. Operasi insert membutuh waktu O(dm) dimana m adalah ukuran string yang baru dan d adalah ukuran alphabet.
Sebagai contoh berikut adalah standard trie yang dibangun dari himpunan string {apel, ayam, baju, baja, bara}.

2.9.2 Suffix Tree
Suffix tree sering digunakan untuk pencarian sequence yang panjang seperti genomes untuk bidang bioinformatik. Pembentukan suffix tree mirip seperti Standard Trie, namun untuk seluruh suffix dalam string. Jika diberikan string dengan panjang n, dibentuk cabang dengan n(n-1)/2 suffix.  Metode ini banyak dimanfaatkan untuk mempercepat proses pencarian jika diberikan sebuah masukan query. Jika terdapat sebuah pattern dengan panjang string m, maka waktu yang dibutuhkan untuk menjalankan proses pattern matching adalah O(dm) dengan d adalah ukuran alfabet. Proses pencarian dilakukan dengan menelusuri path dari root sesuai dengan sequence query. Jika seluruh karakter dalam query selesai dijalankan, maka proses pencarian berhasil. 
Sebagai contoh string ‘babaa’ menghasilkan suffix tree berikut. Jika diberi query ‘ba’ maka akan berhasil terhadap path ‘babaa’ dan ‘baa’.

2.10 Word Embedding
Untuk menentukan similarity antar kata relasi yang dihasilkan dari proses Pattern Matching.

2.11 Pointwise Mutual Information 
Pointwise Mutual Information (PMI) adalah pengukuran asosiasi antar variabel. Dalam bidang information theory, PMI dapat dimanfaatkan untuk menghitung asosiasi kemunculan dua buah kata. Jika diberikan dua buah kata x dan y, maka nilai PMI dapat dihitung menggunakan rumus berikut. 
pmi(x;y)=logp(x,y)p(x)p(y)=logp(x|y)p(x)=logp(y|x)p(y) dengan p(x)=f(x)N
pmi(x;y)=logf(x,y)Nf(x)f(y)
p(x) adalah probabilitas kemunculan kata x dalam korpus
f(x) adalah frekuensi kemunculan kata x dalam korpus
N adalah total seluruh kata dalam korpus
Pengukuran ini bersifat simetris, sehingga p(x;y)=p(y;x). Nilai PMI dapat merupakan bilangan negatif. Jika nilai PMI adalah nol (0) berarti kedua variabel saling independent.

2.11.1 Skip PMI 
PMI umumnya hanya menggunakan model bigram atau trigram. Model ini hanya melihat hubungan kata yang berdampingan. Sebagai contoh ingin diketahui PMI untuk bigram ‘hong kong’, ‘sepak bola’, dan ‘amerika serikat’. Pada penelitian ini dilakukan modifikasi yaitu membuat model skip-gram PMI. Kita menghitung jarak nilai PMI yang berjarak n antar kata. 

2.12 Evaluasi
Evaluasi dilakukan untuk mengetahui kebaikan hasil penelitian. Evaluasi dapat dilakukan dengan mengukur akurasi data yang dihasilkan. Akurasi adalah nilai perbandingan antara jumlah data yang benar dengan jumlah seluruh data. Berikut ada beberapa metode dan teknik evaluasi lain yang digunakan dalam penelitian.

2.12.1 Sampling
Terdapat dua kategori utama dalam sampling yaitu probability dan non-probability sampling. Perbedaan utama keduanya adalah pada probability sampling, diambil data secara acak (random). Dalam probability sampling, terdapat beberapa metode yang dapat digunakan seperti simple random sampling, systematic sampling, stratified random sampling, dan cluster sampling.
Simple random sampling perlu mengetahui seluruh data yang ada dan dari data tersebut dipilih secara acak. Hal ini membuat seluruh data memiliki nilai probabilitas terpilih yang sama. 
Systematic sampling memilih setiap data ke-n untuk dijadikan sample. 
Stratified random sampling akan mengelompokan data ke dalam kategori berdasarkan karakteristik tertentu (strata), kemudian data diambil secara acak dari kategori yang ada. Hal ini menyebabkan hasil lebih representatif. 
Cluster sampling mirip seperti stratified sampling namun dilakukan jika data kelompok yang ingin di-sampling sulit berada di lokasi yang terpisah jauh.
Proses sampling bermanfaat untuk merepresentasikan data tanpa perlu mengevaluasi seluruh data yang ada. Jika jumlah data yang ingin dievaluasi berukuran besar, proses sampling mempercepat pengukuran. Jumlah data yang direpresentasikan oleh satu sample berdasarkan jumlah data asli. Sebagai contoh jika total data adalah 1000 dan jumlah data sample adalah 50, maka satu data sample merepresentasikan 20 data asli.
(Sumber: https://ecduganda.files.wordpress.com/2014/08/how-to-choose-sampling-techniques-for-
evaluations.pdf)
(http://optimierung.mathematik.uni-kl.de/mamaeusch/veroeffentlichungen/ver_texte/sampling_en.pdf)

2.12.2 Precision dan Recall
Teknik yang umum digunakan untuk mengevaluasi suatu ekstraksi adalah precision dan recall. Precision adalah nilai yang menyatakan jumlah dokumen benar dan berhasil diambil dibandingkan dengan seluruh jumlah dokumen yang terambil. Recall adalah nilai yang menyatakan jumlah dokumen benar dan berhasil diambil dibandingkan dengan jumlah seluruh dokumen yang benar. Semakin banyak dokumen yang diambil maka nilai recall akan meningkat sementara nilai precision cenderung menurun. 

2.12.3 Kappa 
Nilai kappa () merepresentasikan tingkat persetujuan antar anotator. Kappa digunakan pada penelitian yang menggunakan bantuan anotator untuk memberi penilaian secara manual. Peniliaian didapatkan menggunakan rumus berikut.
=P(A)-P(E)1-P(E)
P(A) adalah proporsi penilaian yang setuju (agreement)
P(E) adalah proporsi penilaian yang kebetulan
Landis dan Koch [18] mendefiniskan tingkat persetujuan berdasarkan nilai Kappa yang diperoleh. 
Nilai 
Tingkat persetujuan
< 0.00
Poor
0.00 - 0.20
Slight
0.21 - 0.40
Fair
0.41 - 0.60
Moderate
0.61 - 0.80
Substantial
0.81 - 1.00
Almost Perfect
Beberapa variasi perhitungan untuk Kappa adalah Cohen’s Kappa dan Fleiss’ Kappa. Cohen’s Kappa digunakan untuk mengukur tingkat persetujuan antar dua anotator. Jika diberikan data dengan n label dan mij merepresentasikan jumlah data yang diberi label i oleh anotator pertama dan label j oleh anotator kedua, maka proses perhitungan P(A) dan P(E)  untuk Cohen’s Kappa adalah sebagai berikut.
P(A) = k=1nmkktotal data
P(E) =k=1n(j=1nmkj . i = 1nmik)total data 
Fleiss’ Kappa mengukur tingkat persetujuan antar sekelompok anotator berjumlah lebih dari dua. Jika diberikan N data dengan n anotator dimana setiap data diantosi ke dalam salah satu dari k kategori dan nij merepresentasikan total anotator yang memberi data i ke label j, proses perhitungan P(A) dan P(E)  untuk Fleiss’ Kappa adalah sebagai berikut.
P(A) = 1Ni=1NPi dengan Pi=1n(n-1)[(j=1knij2)-(n)]
P(E)=i=1kPi 
[18] (MLA: Landis, J. Richard, and Gary G. Koch. “The Measurement of Observer Agreement for Categorical Data.” Biometrics, vol. 33, no. 1, 1977, pp. 159–174., www.jstor.org/stable/2529310.
APA: Landis, J., & Koch, G. (1977). The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1), 159-174. doi:10.2307/2529310)

2.12.4 Spearman’s Rho
Spearman’s rank correlation coefficient adalah nilai koefisien korelasi antar ranking dua parameter. Nilai Spearman correlation sama dengan nilai Pearson correlation antar dua paramter yang telah di-ranking. Pearson correlation  menggambarkan nilai linear antara dua parameter. Spearman correlation berkisar antara -1 hingga +1.
Spearman’s rho adalah nilai Pearson correlation coefficient antar dua variabel yang telah di-ranking. Untuk mendapatkan nilai koefisien (rs), menggunakan rumus berikut.

adalah Pearson correlation coefficent yang diaplikasikan pada variabel ranking
cov(rgX,rgY) adalah nilai covariance antar variable ranking
rgXdan rgYadalah nilai standard deviasi variable ranking
Jika seluruh ranking berbeda, proses komputasi dapat dilakukan menggunakan rumus berikut.

di=rg(Xi)-rg(Yi)adalah selisih antara dua ranking
nadalah jumlah observasi
